{
    "system": "You are an ambitious AI PhD student eager to publish a high-impact paper on long-horizon time-series forecasting.",
    "task_description": "Design an improved AutoTSF method for multivariate time-series forecasting on the ETT benchmarks. Focus on how to enhance or extend decomposition-based linear models (e.g., DLinear) for long-horizon prediction. Inputs are 7-channel hourly ETTh1 measurements, and the evaluation spans prediction lengths of 96/192/336/720 steps with MAE/MSE as the metrics.",
    "domain": "Long-horizon multivariate time-series forecasting",
    "background": "### 1. Observations and Initial Hypotheses\\n\\n**Key Observations**\\n- Accuracy deteriorates as prediction length grows: vanilla Transformer/Informer baselines often overfit ETTh1 for 720-step horizons, while the simple DLinear baseline reported **MAE 0.438/MSE 0.427** (run_0/final_info.json).\\n- Seasonal-trend mixing is unstable with static convolutions: fixed-size kernels either oversmooth sharp trend reversals (winter/summer shifts) or ignore short bursts (Factory #4 load spikes).\\n- Channel heterogeneity (7 sensors) matters: humidity and oil temperature show different variances; treating all channels with tied weights causes gradient imbalance and underfits minority regimes.\\n\\n**Initial Hypotheses**\\n1. **Adaptive moving-average kernels** that scale with horizon length can stabilize detrending for both 96 and 720 steps.\\n2. **Hybrid shared/individual linear heads** capture cross-channel correlations without exploding parameters.\\n3. **Progressive multi-horizon curriculum** (train short→long) plus frequency-domain regularization reduces exposure bias.\\n\\n**Preliminary Evidence**\\n- Trend/seasonal separation followed by linear extrapolation (DLinear) already surpasses Autoformer on long ETTh1 horizons, hinting that inductive bias, not model size, is critical.\\n- Ablation on channel-wise heads (individual=True) lowered MAE by 5% for channels with low variance, confirming heterogeneous dynamics.\\n- Curriculum schedules that expose the model to (96,192,336,720) progressively improved stability vs. single-horizon training (see internal logbook, Exp_Main).\\n\\n---\\n\\n### 2. Methodological Reasoning and Evolution\\n\\n#### Stage A: Data Conditioning + Tokenization\\n- **Robust scaling**: apply per-channel z-score using only training split to avoid lookahead; improves convergence of the shared backbone.\\n- **Temporal tokens**: concatenate positional encodings (hour, day-of-year, holiday flags) to raw signals; ensures periodic cues survive linear layers.\\n\\n#### Stage B: Adaptive Decomposition Backbone\\n- **Learnable kernel DLinear**: replace fixed kernel size 25 with dynamic kernel k=α·pred_len^β (e.g., α=0.06, β=0.5) so k grows with horizon. Implementation: moving average via depthwise 1D conv with padding to keep length.\\n- **Residual-channel mixer**: after extracting residual R_t and trend T_t, feed R_t to a lightweight frequency mixer (1D FFT → complex gating → iFFT) before the linear head to capture aliasing patterns. Equation: \\n  $$\\hat{R}_f = \\text{Gate}(\\mathcal{F}(R_t)) ⊙ \\mathcal{F}(R_t), \\quad \\hat{R}_t = \\mathcal{F}^{-1}(\\hat{R}_f)$$\\n- **Shared + individual heads**: \\n  $$Y_c = W_{shared} R_c + b_{shared} + W_c^{ind} R_c + b_c^{ind} + W_T T_c$$\\n  where W_c^{ind} only activates for high-variance channels (determined by variance threshold).\\n\\n#### Stage C: Training Strategy\\n- **Curriculum horizons**: loop pred_len in {96,192,336,720}; reuse encoder weights and fine-tune head per horizon (mirrors existing Exp_Main loop).\\n- **Multi-objective loss**: \\(\\mathcal{L}=\\lambda_{MAE}\\text{MAE}+\\lambda_{MSE}\\text{MSE}+\\lambda_{freq}\\|\\mathcal{F}(Y)-\\mathcal{F}(\\hat{Y})\\|_1\\) penalizes frequency drift.\\n- **Stochastic lookback augmentation**: randomly vary seq_len between 256 and 512 to expose multiple temporal contexts without retraining.\\n\\n---\\n\\n### 3. Insights, Interpretations, and Future Directions\\n- **Key Findings**: Adaptive kernels reduced long-horizon MAE by ~7% on ETTh1 (pilot study). Frequency regularization suppressed spurious oscillations beyond 336 steps. Progressive curriculum avoided divergence at 720-step horizon without extra memory.\\n- **Interpretation**: The success of linear decomposers stems from matching inductive bias to seasonal-trend statistics; adding lightweight frequency-domain reasoning preserves this bias while modeling cross-channel dependencies.\\n- **Future Work**: (1) Extend to probabilistic forecasting by predicting mean/variance; (2) Plug into AutoTSF agent loop for automatic hyper search; (3) Transfer to ECL/Weather datasets to verify generality.\\n\\nUse these observations to craft innovative hypotheses, cite relevant literature (e.g., DLinear, TimesNet, FEDformer), and justify how each architectural choice addresses ETTh1 failure modes."
}